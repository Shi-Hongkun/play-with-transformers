# DeepSeek-V3/R1
V3/R1 版本引入了两个关键特性：

## 1. Multi-head Latent Attention（MLA）
MLA 是对传统 Attention 机制的一个突破。与 Multi-head Attention（MHA）或 Grouped-query Attention（GQA）相比，MLA 的设计理念是压缩查询过程中的 Key-Value（KV）缓存数据，从而减少显存使用。


具体方式是在前向传播中**将 KV 向量进行压缩**，存入缓存，推理阶段再通过额外的查询机制还原。这种结构既保证了性能，又能显著降低内存开销。


## 2. 稀疏专家路由（MoE）
DeepSeek-V3 采用了一种大规模稀疏专家架构：**256 个专家模块，每次只激活其中的 9 个专家**，包括一个“共享专家”用于所有 token 的基础处理。总参数量达到 671B，但实际推理时激活参数仅约 37B，大大降低了推理成本，同时保持模型能力的高度可扩展性。


## 此外，
DeepSeek 还对 **LayerNorm 和 KV 缓存**做了定制优化，使其能更好适配高吞吐率场景，如搜索和问答等。


# 三、Gemma 3：为效率而设计的主流架构
Gemma 由 Google DeepMind 发布，属于轻量通用 LLM，专为推理效率和部署友好性而设计，尤其适合中小规模任务和边缘部署场景。

## 1. 滑动窗口注意力（sliding-window attention）
为了在保持长上下文能力的同时降低显存开销，Gemma 采用滑动窗口机制：模型以 5:1 的比例组合局部注意力和全局注意力，并将局部窗口大小从 4k 缩小到 1k。这一设计大幅**减少了 KV 缓存**的存储压力，尤其在多轮对话和流式处理场景中优势明显。


图 11：Gemma 3 论文 （https://arxiv.org/abs/2503.19786） 中的注释图显示了通过滑动窗口注意力节省的 KV 缓存。

图 12：常规注意力（左）和滑动窗口注意力（右）之间的比较。

图 13：来自 Gemma 3 论文 （https://arxiv.org/abs/2503.19786） 的注释图显示，滑动窗口注意力对 LLM 生成的输出困惑度几乎没有影响。
## 2. 双向 RMSNorm
每个模块（包括 attention 和 FFN）**前后都使用 RMSNorm**，是对 LayerNorm 的一种增强，提升模型训练稳定性与泛化能力。



3. Gemma 3n：适配低算力设备
Gemma 还发布了面向低功耗设备的“n”系列版本，采用 PLE（按需加载嵌入）和 MatFormer（可切分 Transformer）技术，进一步降低部署门槛。

# LLaMA 4：旗舰大模型的稀疏化演进

Meta 的 LLaMA 4 参数规模超过 400B，采用了灵活的 MoE 路由机制。

## 1. 双专家激活
与 DeepSeek 每层激活 9 个专家不同，LLaMA 4 每层只激活 2 个专家（从 64 个中选择），使得计算成本与性能之间达到一个新的平衡。

## 2. 层间专家分布不均
LLaMA 在部分层使用密集 FFN，其余层使用 MoE，使模型对低层进行稳定特征提取，对高层进行语义扩展。这种混合架构使训练更稳定，同时提高表达能力。
